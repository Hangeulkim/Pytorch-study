{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (virtualenv)"
  },
  "interpreter": {
   "hash": "ed225720166559e7176d3793db16a2fd8d295f725007103b21ac3099d2a89ee8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.7439110279083252\n",
      "100 0.6932504177093506\n",
      "200 0.6932159066200256\n",
      "300 0.6931926012039185\n",
      "400 0.6931757926940918\n",
      "500 0.6931623220443726\n",
      "600 0.6931504607200623\n",
      "700 0.6931392550468445\n",
      "800 0.6931275725364685\n",
      "900 0.6931143999099731\n",
      "1000 0.6930986642837524\n",
      "1100 0.6930789947509766\n",
      "1200 0.6930529475212097\n",
      "1300 0.6930175423622131\n",
      "1400 0.692966639995575\n",
      "1500 0.6928892135620117\n",
      "1600 0.6927623748779297\n",
      "1700 0.692533552646637\n",
      "1800 0.692061722278595\n",
      "1900 0.6908833980560303\n",
      "2000 0.6869640350341797\n",
      "2100 0.6682999134063721\n",
      "2200 0.584000289440155\n",
      "2300 0.4193096160888672\n",
      "2400 0.20987842977046967\n",
      "2500 0.09931641072034836\n",
      "2600 0.05965084210038185\n",
      "2700 0.04158172011375427\n",
      "2800 0.031611863523721695\n",
      "2900 0.025387229397892952\n",
      "3000 0.021163541823625565\n",
      "3100 0.018122756853699684\n",
      "3200 0.01583496481180191\n",
      "3300 0.014054317027330399\n",
      "3400 0.012630550190806389\n",
      "3500 0.01146705076098442\n",
      "3600 0.010498893447220325\n",
      "3700 0.009681030176579952\n",
      "3800 0.008981173858046532\n",
      "3900 0.008375605568289757\n",
      "4000 0.007846543565392494\n",
      "4100 0.0073804003186523914\n",
      "4200 0.0069666290655732155\n",
      "4300 0.006596860010176897\n",
      "4400 0.006264477502554655\n",
      "4500 0.0059640854597091675\n",
      "4600 0.005691260099411011\n",
      "4700 0.005442371126264334\n",
      "4800 0.005214452277868986\n",
      "4900 0.005004897713661194\n",
      "5000 0.0048116398975253105\n",
      "5100 0.004632809665054083\n",
      "5200 0.004466845653951168\n",
      "5300 0.00431243609637022\n",
      "5400 0.004168355371803045\n",
      "5500 0.004033683333545923\n",
      "5600 0.0039074718952178955\n",
      "5700 0.003788930829614401\n",
      "5800 0.0036774585023522377\n",
      "5900 0.003572340589016676\n",
      "6000 0.003473140997812152\n",
      "6100 0.003379273694008589\n",
      "6200 0.0032904527615755796\n",
      "6300 0.00320613756775856\n",
      "6400 0.003126079449430108\n",
      "6500 0.0030499265994876623\n",
      "6600 0.0029774606227874756\n",
      "6700 0.0029083136469125748\n",
      "6800 0.002842365764081478\n",
      "6900 0.002779376693069935\n",
      "7000 0.002719106851145625\n",
      "7100 0.002661427715793252\n",
      "7200 0.0026061302050948143\n",
      "7300 0.00255311606451869\n",
      "7400 0.0025022211484611034\n",
      "7500 0.002453324617817998\n",
      "7600 0.0024063296150416136\n",
      "7700 0.0023610710632056\n",
      "7800 0.0023175484966486692\n",
      "7900 0.0022755831014364958\n",
      "8000 0.0022351061925292015\n",
      "8100 0.0021960516460239887\n",
      "8200 0.0021583582274615765\n",
      "8300 0.002121951896697283\n",
      "8400 0.00208674231544137\n",
      "8500 0.0020527145825326443\n",
      "8600 0.002019741805270314\n",
      "8700 0.0019878605380654335\n",
      "8800 0.0019569594878703356\n",
      "8900 0.0019270003540441394\n",
      "9000 0.0018979613669216633\n",
      "9100 0.0018697894411161542\n",
      "9200 0.0018424253212288022\n",
      "9300 0.0018158985767513514\n",
      "9400 0.0017900895327329636\n",
      "9500 0.0017650282243266702\n",
      "9600 0.0017406617989763618\n",
      "9700 0.001716946135275066\n",
      "9800 0.0016938732005655766\n",
      "9900 0.0016714207595214248\n",
      "10000 0.0016495509771630168\n",
      "\n",
      "Hypothesis:  [[2.0288344e-04]\n",
      " [9.9794775e-01]\n",
      " [9.9785233e-01]\n",
      " [2.3444067e-03]\n",
      " [9.9725562e-01]\n",
      " [1.8365671e-03]\n",
      " [1.7079324e-03]\n",
      " [9.9985540e-01]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "### 2 input xor이 아닌 3 input xor로 변경 은닉층의 갯수에 따라 결과가 달라짐\n",
    "import torch\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "X = torch.FloatTensor([[0, 0,0], [0, 0,1], [0, 1,0], [0,1, 1],[1, 0,0],[1, 0,1],[1, 1,0],[1, 1,1],]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0],[1], [0], [0], [1]]).to(device)\n",
    "\n",
    "linear1 = torch.nn.Linear(3,3,bias=True)\n",
    "linear2 = torch.nn.Linear(3,1,bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 2.1997134685516357\n",
      "100 0.6431387066841125\n",
      "200 0.5664221048355103\n",
      "300 0.5105169415473938\n",
      "400 0.46477317810058594\n",
      "500 0.4242010712623596\n",
      "600 0.3850454092025757\n",
      "700 0.3463992774486542\n",
      "800 0.30894935131073\n",
      "900 0.27389979362487793\n",
      "1000 0.24227359890937805\n",
      "1100 0.21456950902938843\n",
      "1200 0.19077196717262268\n",
      "1300 0.17054003477096558\n",
      "1400 0.15339647233486176\n",
      "1500 0.13885094225406647\n",
      "1600 0.12645864486694336\n",
      "1700 0.11584067344665527\n",
      "1800 0.10668490827083588\n",
      "1900 0.09873753786087036\n",
      "2000 0.09179384261369705\n",
      "2100 0.08568848669528961\n",
      "2200 0.08028796315193176\n",
      "2300 0.07548407465219498\n",
      "2400 0.07118801772594452\n",
      "2500 0.06732720881700516\n",
      "2600 0.06384152173995972\n",
      "2700 0.060681067407131195\n",
      "2800 0.05780404061079025\n",
      "2900 0.05517538636922836\n",
      "3000 0.05276529863476753\n",
      "3100 0.05054834485054016\n",
      "3200 0.04850300773978233\n",
      "3300 0.04661067575216293\n",
      "3400 0.04485523700714111\n",
      "3500 0.043222662061452866\n",
      "3600 0.04170087352395058\n",
      "3700 0.04027919843792915\n",
      "3800 0.038948263972997665\n",
      "3900 0.037699975073337555\n",
      "4000 0.03652677685022354\n",
      "4100 0.035422466695308685\n",
      "4200 0.034381210803985596\n",
      "4300 0.03339774161577225\n",
      "4400 0.032467566430568695\n",
      "4500 0.031586579978466034\n",
      "4600 0.03075101226568222\n",
      "4700 0.029957417398691177\n",
      "4800 0.029202785342931747\n",
      "4900 0.028484459966421127\n",
      "5000 0.02779988944530487\n",
      "5100 0.02714666724205017\n",
      "5200 0.02652287855744362\n",
      "5300 0.025926485657691956\n",
      "5400 0.025355830788612366\n",
      "5500 0.024809278547763824\n",
      "5600 0.02428528666496277\n",
      "5700 0.023782595992088318\n",
      "5800 0.023299938067793846\n",
      "5900 0.022836055606603622\n",
      "6000 0.02238999679684639\n",
      "6100 0.021960802376270294\n",
      "6200 0.02154739759862423\n",
      "6300 0.021149029955267906\n",
      "6400 0.020764905959367752\n",
      "6500 0.02039424702525139\n",
      "6600 0.020036395639181137\n",
      "6700 0.019690684974193573\n",
      "6800 0.019356537610292435\n",
      "6900 0.019033275544643402\n",
      "7000 0.01872062310576439\n",
      "7100 0.018417801707983017\n",
      "7200 0.018124569207429886\n",
      "7300 0.017840387299656868\n",
      "7400 0.017564821988344193\n",
      "7500 0.0172975342720747\n",
      "7600 0.01703810505568981\n",
      "7700 0.016786284744739532\n",
      "7800 0.016541723161935806\n",
      "7900 0.016304073855280876\n",
      "8000 0.016073063015937805\n",
      "8100 0.015848413109779358\n",
      "8200 0.015629906207323074\n",
      "8300 0.01541723683476448\n",
      "8400 0.015210248529911041\n",
      "8500 0.015008663758635521\n",
      "8600 0.014812299981713295\n",
      "8700 0.014620926231145859\n",
      "8800 0.01443440094590187\n",
      "8900 0.014252439141273499\n",
      "9000 0.014075031504034996\n",
      "9100 0.0139019675552845\n",
      "9200 0.013732990249991417\n",
      "9300 0.013568015769124031\n",
      "9400 0.013406986370682716\n",
      "9500 0.013249656185507774\n",
      "9600 0.013095933943986893\n",
      "9700 0.01294578518718481\n",
      "9800 0.012798878364264965\n",
      "9900 0.012655272148549557\n",
      "10000 0.012514873407781124\n",
      "\n",
      "Hypothesis:  [[0.01470234]\n",
      " [0.99061894]\n",
      " [0.99061894]\n",
      " [0.01625817]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "device = 'cpu'\n",
    "torch.manual_seed(777)\n",
    "\n",
    "X = torch.FloatTensor([[ 0,0], [ 0,1], [ 1,0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "w1 = torch.Tensor(2,2).to(device)\n",
    "b1 = torch.Tensor(2).to(device)\n",
    "w2 = torch.Tensor(2,1).to(device)\n",
    "b2 = torch.Tensor(1).to(device)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "for step in range(10001):\n",
    "    l1 = torch.add(torch.matmul(X,w1),b1)\n",
    "    a1 = sigmoid(l1)\n",
    "    l2 = torch.add(torch.matmul(a1,w2),b2)\n",
    "    Y_pred = sigmoid(l2)\n",
    "\n",
    "    cost = -torch.mean(Y*torch.log(Y_pred)+(1-Y)*torch.log(1-Y_pred))\n",
    "\n",
    "    d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)\n",
    "\n",
    "    d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
    "    d_b2 = d_l2\n",
    "    d_w2 = torch.matmul(torch.transpose(a1,0,1),d_b2)\n",
    "\n",
    "    d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
    "    d_l1 = d_a1 * sigmoid_prime(l1)\n",
    "    d_b1 = d_l1\n",
    "    d_w1 = torch.matmul(torch.transpose(X,0,1), d_b1)\n",
    "\n",
    "    w1 = w1 - learning_rate *d_w1\n",
    "    b1 = b1 - learning_rate * torch.mean(d_b1,0)\n",
    "    w2 = w2 - learning_rate *d_w2\n",
    "    b2 = b2 - learning_rate * torch.mean(d_b2,0)\n",
    "\n",
    "    if step%100 == 0:\n",
    "        print(step, cost.item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    l1 = torch.add(torch.matmul(X,w1),b1)\n",
    "    a1 = sigmoid(l1)\n",
    "    l2 = torch.add(torch.matmul(a1,w2),b2)\n",
    "    hypothesis = sigmoid(l2)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.7012462019920349\n",
      "100 0.6931397914886475\n",
      "200 0.6931383013725281\n",
      "300 0.6931366920471191\n",
      "400 0.693135142326355\n",
      "500 0.6931335926055908\n",
      "600 0.6931319236755371\n",
      "700 0.6931301355361938\n",
      "800 0.6931282877922058\n",
      "900 0.6931264400482178\n",
      "1000 0.6931244134902954\n",
      "1100 0.6931223273277283\n",
      "1200 0.6931200623512268\n",
      "1300 0.6931177973747253\n",
      "1400 0.693115234375\n",
      "1500 0.6931125521659851\n",
      "1600 0.6931096315383911\n",
      "1700 0.6931065917015076\n",
      "1800 0.6931031942367554\n",
      "1900 0.6930996179580688\n",
      "2000 0.6930956840515137\n",
      "2100 0.6930913329124451\n",
      "2200 0.6930866241455078\n",
      "2300 0.6930813789367676\n",
      "2400 0.6930755376815796\n",
      "2500 0.6930691003799438\n",
      "2600 0.6930618286132812\n",
      "2700 0.693053662776947\n",
      "2800 0.6930443644523621\n",
      "2900 0.6930336952209473\n",
      "3000 0.6930214762687683\n",
      "3100 0.6930070519447327\n",
      "3200 0.692990243434906\n",
      "3300 0.6929700970649719\n",
      "3400 0.6929457783699036\n",
      "3500 0.6929160356521606\n",
      "3600 0.6928787231445312\n",
      "3700 0.6928312182426453\n",
      "3800 0.6927690505981445\n",
      "3900 0.6926849484443665\n",
      "4000 0.6925672888755798\n",
      "4100 0.6923938989639282\n",
      "4200 0.6921221017837524\n",
      "4300 0.691658079624176\n",
      "4400 0.6907625794410706\n",
      "4500 0.6886777877807617\n",
      "4600 0.6820249557495117\n",
      "4700 0.6441338658332825\n",
      "4800 0.28177428245544434\n",
      "4900 0.03786884993314743\n",
      "5000 0.014747995883226395\n",
      "5100 0.008498415350914001\n",
      "5200 0.0057906899601221085\n",
      "5300 0.004322083201259375\n",
      "5400 0.00341460551135242\n",
      "5500 0.002804103074595332\n",
      "5600 0.002368092304095626\n",
      "5700 0.002042594598606229\n",
      "5800 0.0017911759205162525\n",
      "5900 0.0015916433185338974\n",
      "6000 0.0014298022724688053\n",
      "6100 0.0012961144093424082\n",
      "6200 0.001183958724141121\n",
      "6300 0.0010886357631534338\n",
      "6400 0.0010067173279821873\n",
      "6500 0.0009356046793982387\n",
      "6600 0.0008733489084988832\n",
      "6700 0.0008184199687093496\n",
      "6800 0.0007696531247347593\n",
      "6900 0.0007260260172188282\n",
      "7000 0.000686837185639888\n",
      "7100 0.0006514150300063193\n",
      "7200 0.0006193265435285866\n",
      "7300 0.0005900647374801338\n",
      "7400 0.0005632863030768931\n",
      "7500 0.0005386928096413612\n",
      "7600 0.0005160978180356324\n",
      "7700 0.0004951879964210093\n",
      "7800 0.00047583659761585295\n",
      "7900 0.0004578570951707661\n",
      "8000 0.0004411078989505768\n",
      "8100 0.00042546208715066314\n",
      "8200 0.00041086753481067717\n",
      "8300 0.0003971750847995281\n",
      "8400 0.00038434003363363445\n",
      "8500 0.0003722578985616565\n",
      "8600 0.0003608616243582219\n",
      "8700 0.0003500990860629827\n",
      "8800 0.0003399328561499715\n",
      "8900 0.00033032571082003415\n",
      "9000 0.00032121059484779835\n",
      "9100 0.00031255767680704594\n",
      "9200 0.0003043593605980277\n",
      "9300 0.00029651893419213593\n",
      "9400 0.00028906611260026693\n",
      "9500 0.00028196361381560564\n",
      "9600 0.0002751890860963613\n",
      "9700 0.00026869779685512185\n",
      "9800 0.000262526998994872\n",
      "9900 0.0002565947943367064\n",
      "10000 0.00025093829026445746\n",
      "\n",
      "Hypothesis:  [[2.5580099e-04]\n",
      " [9.9970204e-01]\n",
      " [9.9975365e-01]\n",
      " [2.8076881e-04]\n",
      " [9.9970978e-01]\n",
      " [1.9024074e-04]\n",
      " [2.8128485e-04]\n",
      " [9.9983585e-01]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "### 2 input xor이 아닌 3 input xor로 변경 은닉층의 갯수에 따라 결과가 달라짐\n",
    "### 은닉층이 너무 많은경우 오히려 결과가 제대로 나오지 않음## 2 input xor이 아닌 3 input xor로 변경 은닉층의 갯수에 따라 결과가 달라짐\n",
    "### 은닉층이 너무 많은경우 오히려 결과가 제대로 나오지 않음\n",
    "import torch\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "\n",
    "X = torch.FloatTensor([[0, 0,0], [0, 0,1], [0, 1,0], [0,1, 1],[1, 0,0],[1, 0,1],[1, 1,0],[1, 1,1],]).to(device)\n",
    "\n",
    "import torch\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "\n",
    "X = torch.FloatTensor([[0, 0,0], [0, 0,1], [0, 1,0], [0,1, 1],[1, 0,0],[1, 0,1],[1, 1,0],[1, 1,1],]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0],[1], [0], [0], [1]]).to(device)\n",
    "\n",
    "linear1 = torch.nn.Linear(3,10,bias=True)\n",
    "linear2 = torch.nn.Linear(10,10,bias=True)\n",
    "linear3 = torch.nn.Linear(10,1,bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid).to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}